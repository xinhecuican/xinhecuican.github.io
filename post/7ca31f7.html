<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>神经网络（NN） | Xinhecuican's Blog</title><meta name="author" content="星河璀璨"><meta name="copyright" content="星河璀璨"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="多层向前神经网络 该神经网络的层数大的有三层：输入层， 隐藏层（隐藏层可以有多层）， 输出层。 该图是两层神经网络（输入层不算） 每层由单元组成（例如决策树算法中的一和零）	。输入层就是传入一些特征向量。 理解： 下一层的神经元可以看成y，然后每个w可以看成k，那么其实就是一条直线。有些层次用来做 &amp;&amp; 或 || 的操作，这样就可以用多条直线对区域进行划分. 权重： 每两层有线进">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络（NN）">
<meta property="og:url" content="https://xinhecuican.github.io/post/7ca31f7.html">
<meta property="og:site_name" content="Xinhecuican&#39;s Blog">
<meta property="og:description" content="多层向前神经网络 该神经网络的层数大的有三层：输入层， 隐藏层（隐藏层可以有多层）， 输出层。 该图是两层神经网络（输入层不算） 每层由单元组成（例如决策树算法中的一和零）	。输入层就是传入一些特征向量。 理解： 下一层的神经元可以看成y，然后每个w可以看成k，那么其实就是一条直线。有些层次用来做 &amp;&amp; 或 || 的操作，这样就可以用多条直线对区域进行划分. 权重： 每两层有线进">
<meta property="og:locale">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2020-07-24T02:35:00.000Z">
<meta property="article:modified_time" content="2020-08-23T03:25:10.604Z">
<meta property="article:author" content="星河璀璨">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><!--link(rel="canonical" href=urlNoIndex())--><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="zmX9zYZL0QVB6fFG_e98QsQogwE11Vdf5hOd4xMbgN4"><meta name="baidu-site-verification" content="EVZv9BO5R3"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-08-23 11:25:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="footer.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Xinhecuican's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">223</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">6</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">33</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Xinhecuican's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">神经网络（NN）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-07-24T02:35:00.000Z" title="Created 2020-07-24 10:35:00">2020-07-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-08-23T03:25:10.604Z" title="Updated 2020-08-23 11:25:10">2020-08-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>11min</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="多层向前神经网络"><a class="markdownIt-Anchor" href="#多层向前神经网络"></a> 多层向前神经网络</h1>
<p>该神经网络的层数大的有三层：输入层， 隐藏层（隐藏层可以有多层）， 输出层。</p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1.png" alt>该图是两层神经网络（输入层不算）</p>
<p>每层由单元组成（例如决策树算法中的一和零）	。输入层就是传入一些特征向量。</p>
<p><strong>理解</strong>：</p>
<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line">下一层的神经元可以看成y，然后每个w可以看成k，那么其实就是一条直线。有些层次用来做 <span class="meta">&amp;&amp; 或 || 的操作，这样就可以用多条直线对区域进行划分.</span></span><br></pre></td></tr></table></figure>
<p>权重： 每两层有线进行连接，线上的数值就是权重，我们是通过特征向量和权重相乘求和再用非线性方程转化得到下一层的</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span>. 设计神经网络结构</span><br><span class="line">     <span class="number">3</span>.<span class="number">1</span> 使用神经网络训练数据之前，必须确定神经网络的层数，以及每层单元的个数</span><br><span class="line">     <span class="number">3</span>.<span class="number">2</span> 特征向量在被传入输入层时通常被先标准化(<span class="keyword">normalize）到0和1之间 </span>（为了加速学习过程）</span><br><span class="line">     <span class="number">3</span>.<span class="number">3</span> 离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值</span><br><span class="line">          比如：特征值A可能取三个值（<span class="built_in">a0</span>, <span class="built_in">a1</span>, <span class="built_in">a2</span>), 可以使用<span class="number">3</span>个输入单元来代表A。</span><br><span class="line">                    如果A=<span class="built_in">a0</span>, 那么代表<span class="built_in">a0</span>的单元值就取<span class="number">1</span>, 其他取<span class="number">0</span>；</span><br><span class="line">                    如果A=<span class="built_in">a1</span>, 那么代表a1de单元值就取<span class="number">1</span>，其他取<span class="number">0</span>，以此类推</span><br><span class="line"></span><br><span class="line">     <span class="number">3</span>.<span class="number">4</span> 神经网络即可以用来做分类(classification）问题，也可以解决回归(regression)问题</span><br><span class="line">          <span class="number">3</span>.<span class="number">4</span>.<span class="number">1</span> 对于分类问题，如果是<span class="number">2</span>类，可以用一个输出单元表示（<span class="number">0</span>和<span class="number">1</span>分别代表<span class="number">2</span>类,例如黑和白，不是黑就是白，所以只需要输出一类）</span><br><span class="line">                   如果多于<span class="number">2</span>类，每一个类别用一个输出单元表示</span><br><span class="line">                   所以输入层的单元数量通常等于类别的数量</span><br><span class="line"></span><br><span class="line">          <span class="number">3</span>.<span class="number">4</span>.<span class="number">2</span> 没有明确的规则来设计最好有多少个隐藏层</span><br><span class="line">                    <span class="number">3</span>.<span class="number">4</span>.<span class="number">2</span>.<span class="number">1</span> 根据实验测试和误差，以及准确度来实验并改进</span><br></pre></td></tr></table></figure>
<p><strong>交叉验证方法</strong></p>
<p>这是一种验证正确率的方法。例如我们把样例集分成10份，第一次用第一份做测试集，其他做训练集，第二次用第二个做测试集，其他做训练集。这样做十次得到的正确率再求平均值。当然划分不一定是十份。</p>
<p><strong>神经网络训练大致过程</strong>： 先根据输入确定结果，通过预测结果和真实结果之间的误差反推更新权重。</p>
<p>开始的时候可以随机的在1到-1之间给权重</p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2.png" alt></p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C3.png" alt></p>
<p>下面这个式子就是从下一层的计算公式，单元值乘以权重求和然后再加上偏向(oj)</p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C4.png" alt></p>
<p>前面到Bias的部分已经提到了，就是上面那个方程，最后还需要经过一个非线性函数（激活函数）。</p>
<p>激活函数：</p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C5.png" alt></p>
<p>其中Ij就是前面提到的函数。</p>
<p>之后反向更新权重：</p>
<p>对于输出层：<br>
<img src="/images/pasted-10.png" alt>Tj是输出层标签真实值</p>
<p>对于隐藏层：<br>
<img src="/images/pasted-11.png" alt>其中Errk是前面一层的误差</p>
<p>权重更新：</p>
<p><img src="/images/pasted-12.png" alt></p>
<p>l是学习率（learning weight），这是我们手工设置的值，在零到一之间</p>
<p>偏向更新：<br>
<img src="/images/pasted-13.png" alt="upload successful"></p>
<p>终止条件：</p>
<ul>
<li>权重的更新低于某个阈值</li>
<li>
<pre><code>         预测的错误率低于某个阈值
</code></pre>
</li>
<li>
<pre><code>         达到预设一定的循环次数
</code></pre>
</li>
</ul>
<h2 id="梯度下降算法"><a class="markdownIt-Anchor" href="#梯度下降算法"></a> 梯度下降算法</h2>
<p>数学中梯度指的是函数关于各个偏导的一个向量，它的意义是指向上升最快的方向。因此负梯度就是下降最快的方向。</p>
<p>梯度下降算法的基本思想就是沿着梯度每次走一定距离，然后再次计算梯度，重复步骤直到走到最低点。这里的最低点是极值而不是最值</p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C14.jpg" alt>以二维为例。如果让x在最低点左边。x-梯度（导数），那么x增大，朝着最低点靠近。如果在右边x-梯度，x减小，同样朝着最低点靠近。</p>
<p>我们是根据loss function来对神经网络进行调整的。而lossfunction的参数就是w和bias，因此可以对w和bias求偏导然后w- 偏导对w进行修正。</p>
<p><strong>随机梯度下降算法</strong>： 多次随机选取一些样本（mini-batch)，直到所有样例都被选取。</p>
<h2 id="反向更新"><a class="markdownIt-Anchor" href="#反向更新"></a> 反向更新</h2>
<p>反向更新利用了梯度下降算法。也就是使用 w = wi - (delta)w的方式进行更新。</p>
<p>(delta)w = L * 偏Cost / 偏w ， 所以我们的目标就是要求出偏导。</p>
<p>反向更新主要用到了四个式子。<br>
<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C15.PNG" alt></p>
<p>这几个式子都有证明，这里不详细描述。</p>
<ul>
<li>第三个和第四个式子就是偏导，我们看到其中的量可以通过第一个和第二个方程求出来。</li>
<li>第一个式子是对于输出层来说的。右边第一项指的是cost关于a（预测值）的偏导。这里cost的计算式为 (预测值-实际值)的平方求和再除以2n。因此偏导就是预测值减去实际值。后面一项是激活函数的导数。</li>
<li>第二个式子是对于其他层。</li>
</ul>
<p>这里还有另一种cost <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C16.PNG" alt></p>
<p>它的偏导数为<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C17.PNG" alt> <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C18.PNG" alt>.</p>
<p>这个偏导数好在偏导的大小由a-y决定。a-y其实就是error。 error大，下降就要快。</p>
<p><strong>推导过程</strong></p>
<p>其实反向更新就是求偏导 <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C27.png" alt><br>
这张图表示的是从输出反向推第一个权重，也就是上面的第一个式子。</p>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">net</span> = w * x + b</span><br><span class="line"><span class="keyword">out</span> = 1 / 1 + <span class="keyword">e</span>^(-<span class="keyword">net</span>)</span><br><span class="line">∂<span class="keyword">E</span> / ∂<span class="keyword">out</span> = target - <span class="keyword">out</span></span><br><span class="line">∂<span class="keyword">out</span> / ∂<span class="keyword">net</span> = <span class="keyword">net</span> * (1 - <span class="keyword">net</span>)</span><br><span class="line">∂<span class="keyword">net</span> / ∂w = x</span><br><span class="line">∂<span class="keyword">E</span> / ∂w = (target - <span class="keyword">out</span>) * <span class="keyword">net</span> * (1 - <span class="keyword">net</span>) * x</span><br></pre></td></tr></table></figure>
<p>再看另一个例子<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C28.png" alt>这里就可以解释为什么有个求和的过程了，到w不只有一条路，而这里多了两个偏导∂outh1 / ∂neth1 和 ∂neth1 / ∂w1.这里的数值和前面是一样的。</p>
<p>有一点和前面不同，前面是∂E / ∂w，这里是∂E / ∂outh1。所以前面最后乘了一个x而这里乘了一个w。</p>
<h2 id="非线性转化函数"><a class="markdownIt-Anchor" href="#非线性转化函数"></a> 非线性转化函数</h2>
<p>激活函数一般使用S型曲线（sigmoid）。一般是双曲函数(tanh)或逻辑函数。</p>
<p>广义上的sigmoid函数需要在-1到1之间变化并且平滑。</p>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C7.PNG" alt></p>
<p><strong>双曲函数</strong>：</p>
<p>tanhx = sinhx/coshx =</p>
<p>sinhx = (e^x - e^(-x))/2 、 coshx = (e^x + e^(-x)) / 2</p>
<p>导数： <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C10.PNG" alt></p>
<p>图像为：<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C8.PNG" alt></p>
<p><strong>逻辑函数</strong></p>
<p>p(t) = 1/(1 + e^(-t))</p>
<p>导数： <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C11.PNG" alt></p>
<p>图像为： <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C9.PNG" alt></p>
<h2 id="减小overfitting"><a class="markdownIt-Anchor" href="#减小overfitting"></a> 减小overfitting</h2>
<ol>
<li>
<p>增加训练数据集</p>
</li>
<li>
<p>减神经网络的规模</p>
</li>
<li>
<p>regularization <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C21.PNG" alt>.这是一个例子，后面一项也可以应用于其他cost函数中。加了这一项后神经网络会倾向于学习较小的权重,更少可能受到局部噪音影响<br>
<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C22.PNG" alt>w的更新也有变化<br>
<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C23.PNG" alt> nameda随着n的变化而变化，目的是不让比值太小从而使作用失效</p>
<p>另一种regularization函数 <img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C24.PNG" alt>它的偏导数为<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C25.PNG" alt>注意当w=0时，w不可导，所以直接使用没有regularization的。</p>
</li>
<li>
<ol start="3">
<li>DropOut： 防止过拟合。具体方法时让需要dropout的层的百分之p的神经元关闭（即让需要关闭的神经元值为0）。然后多次随机剔除，最后再把权重除以p</li>
</ol>
</li>
<li>
<p><strong>softmax</strong></p>
</li>
</ol>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C19.PNG" alt></p>
<p>这个函数有一些独特的性质。例如zj增大，那么对应输出增大，其他输出减小。并且同一层所有输出值的和一定是1，可以用来模拟概率。所以经常用在输出层当做概率</p>
<p>对应我们可以定义一个新的cost函数 Cost(p,q)=−∑xp(x)logq(x)， 其中p是真实值，q是估计值. 它的偏导为<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C20.PNG" alt>这个偏导和上面的cross-entropy类似。</p>
<h1 id="卷积神经网络"><a class="markdownIt-Anchor" href="#卷积神经网络"></a> 卷积神经网络</h1>
<p>卷积神经网络对隐藏层进行了细分，常用于对图像处理。</p>
<ul>
<li>input layer： 还需要对输入数据进行一些处理，如减去均值（只需要使用训练集上均值，测试集也是使用训练集均值）</li>
<li>卷积计算层（CONV layer）： 通过一个窗口进行移动然后再和w矩阵进行点乘过滤一些信息。有三个主要参数，深度，步长和填充值。深度指的是下一层神经元数目，步长指的是窗口一次移动的长度，填充值是为了防止移动超出范围在周围补的一圈零。</li>
</ul>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C6.PNG" alt>这里深度是2，步长是2，填充值是1.其中最右边绿色就是输出。它是通过左边蓝色的窗口和红色的w进行点乘然后相加得到的。有三层是因为输入一个32 * 32 * 3（RGB）的矩阵，然后通过运算可以得到输出矩阵。</p>
<ul>
<li>激励层： 将卷积层结果进行非线性映射，典型的激励函数是ReLu，sigmoid函数其实很少用了，因为在数据比较大的时候导数趋近于0，难以训练。</li>
</ul>
<p>ReLu方程式 y = max(0, x).也就是小于0时y=0，大于0时y=x。但是这个函数问题是小于0时导数=0，也无法训练。因此改进是小于0时y=0.01x</p>
<ul>
<li>池化层（pooling layer): 池化层一般夹在连续的卷积层中间，它是用来压缩数据量，减少过拟合。方法是max pooling。也是通过一个窗口，每次去窗口中的最大值形成一个矩阵。</li>
</ul>
<p><img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C12.PNG" alt>由原来的4 * 4矩阵变成2 * 2矩阵</p>
<ul>
<li>全连接层： 该层和前一层之间一般所有神经元都有权重连接，一般是放在神经网络尾部，是用来防止信息丢失太多的。</li>
</ul>
<p><strong>注意点</strong></p>
<ol>
<li>
<p>权重初始化： 使用高斯函数（正态分布）去随机初始化可以让权重随机化。也就是numpy.random.randn(in, out) / np.sqrt(in/2). in是输入层个数，out是输出层个数</p>
</li>
<li>
<p>Batch Normalization: 它是用来减少初始值依赖的，通常在全连接层之后。具体算法<img src="/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C13.PNG" alt>其中y和b是神经网络自己学习的。</p>
</li>
</ol>
<h1 id="实现"><a class="markdownIt-Anchor" href="#实现"></a> 实现</h1>
<p>通过前面逻辑函数的导数可以得知，前面权重更新其实就是运用激活函数的导数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh_deriv</span>(<span class="params">x</span>):</span> <span class="comment"># 导数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - np.tanh(x)*np.tanh(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_derivative</span>(<span class="params">x</span>):</span> <span class="comment"># 导数</span></span><br><span class="line">    <span class="keyword">return</span> logistic(x)*(<span class="number">1</span>-logistic(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layers, activation=<span class="string">&#x27;tanh&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param layers: A list containing the number of units in each layer.</span></span><br><span class="line"><span class="string">        Should be at least two values</span></span><br><span class="line"><span class="string">        :param activation: The activation function to be used. Can be</span></span><br><span class="line"><span class="string">        &quot;logistic&quot; or &quot;tanh&quot;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># layers是一个列表，代表输入层，隐藏层和输出层，列表中每个数字代表隐藏层中单元的数目，列表长度代表总共有多少层</span></span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">&#x27;logistic&#x27;</span>:</span><br><span class="line">            self.activation = logistic</span><br><span class="line">            self.activation_deriv = logistic_derivative</span><br><span class="line">        <span class="keyword">elif</span> activation == <span class="string">&#x27;tanh&#x27;</span>:</span><br><span class="line">            self.activation = tanh</span><br><span class="line">            self.activation_deriv = tanh_deriv</span><br><span class="line"></span><br><span class="line">        self.weights = [] <span class="comment"># 初始化一个列表来装所有的weight，这后来是一个三重矩阵</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(layers) - <span class="number">1</span>): <span class="comment"># 随机初始化weight</span></span><br><span class="line">            self.weights.append((<span class="number">2</span>*np.random.random((layers[i - <span class="number">1</span>] + <span class="number">1</span>, layers[i] + <span class="number">1</span>))-<span class="number">1</span>)*<span class="number">0.25</span>) <span class="comment"># 生成一个layers[i-1]+1行，layer[i]+1列的范围在-0.25到0.25的矩阵</span></span><br><span class="line">            self.weights.append((<span class="number">2</span>*np.random.random((layers[i] + <span class="number">1</span>, layers[i + <span class="number">1</span>]))-<span class="number">1</span>)*<span class="number">0.25</span>) <span class="comment"># 第i层到第i+1层之间</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y, learning_rate=<span class="number">0.2</span>, epochs=<span class="number">10000</span></span>):</span></span><br><span class="line">    <span class="comment"># epochs是训练次数（循环次数）</span></span><br><span class="line">        X = np.atleast_2d(X)</span><br><span class="line">        temp = np.ones([X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]+<span class="number">1</span>])<span class="comment"># 初始化一个矩阵，值全是一，行数和列数由参数提供</span></span><br><span class="line">        temp[:, <span class="number">0</span>:-<span class="number">1</span>] = X  <span class="comment"># adding the bias unit to the input layer</span></span><br><span class="line">        X = temp</span><br><span class="line">        y = np.array(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            i = np.random.randint(X.shape[<span class="number">0</span>])</span><br><span class="line">            a = [X[i]]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.weights)):  <span class="comment">#going forward network, for each layer</span></span><br><span class="line">                a.append(self.activation(np.dot(a[l], self.weights[l])))  <span class="comment">#Computer the node value for each layer (O_i) using activation function</span></span><br><span class="line">              <span class="comment">#正向所有更新</span></span><br><span class="line">            error = y[i] - a[-<span class="number">1</span>]  <span class="comment">#Computer the error at the top layer</span></span><br><span class="line">            deltas = [error * self.activation_deriv(a[-<span class="number">1</span>])] <span class="comment">#For output layer, Err calculation (delta is updated error)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#Staring backprobagation</span></span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a) - <span class="number">2</span>, <span class="number">0</span>, -<span class="number">1</span>): <span class="comment"># we need to begin at the second to last layer</span></span><br><span class="line">                <span class="comment">#Compute the updated error (i,e, deltas) for each node going from top layer to input layer</span></span><br><span class="line"></span><br><span class="line">                deltas.append(deltas[-<span class="number">1</span>].dot(self.weights[l].T)*self.activation_deriv(a[l]))</span><br><span class="line">            deltas.reverse()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.weights)):</span><br><span class="line">                layer = np.atleast_2d(a[i])</span><br><span class="line">                delta = np.atleast_2d(deltas[i])</span><br><span class="line">                self.weights[i] += learning_rate * layer.T.dot(delta)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = np.array(x)</span><br><span class="line">        temp = np.ones(x.shape[<span class="number">0</span>]+<span class="number">1</span>)</span><br><span class="line">        temp[<span class="number">0</span>:-<span class="number">1</span>] = x</span><br><span class="line">        a = temp</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.weights)):</span><br><span class="line">            a = self.activation(np.dot(a, self.weights[l]))</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" rel="external nofollow noopener noreferrer" target="_blank">星河璀璨</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xinhecuican.github.io/post/7ca31f7.html">https://xinhecuican.github.io/post/7ca31f7.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/40997091.html"><img class="prev-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">线性回归</div></div></a></div><div class="next-post pull-right"><a href="/post/34811d5f.html"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">贪心与k-优化</div></div></a></div></nav><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E5%90%91%E5%89%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text"> 多层向前神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text"> 梯度下降算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E6%9B%B4%E6%96%B0"><span class="toc-number">1.2.</span> <span class="toc-text"> 反向更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%BD%AC%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.</span> <span class="toc-text"> 非线性转化函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%8F%E5%B0%8Foverfitting"><span class="toc-number">1.4.</span> <span class="toc-text"> 减小overfitting</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text"> 卷积神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text"> 实现</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By 星河璀璨</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'UJ3adzdUjnlMT2KmVU6dfoyO-MdYXbMMI',
      appKey: 'SGFIXvul1h4NATEJbvSGuU0i',
      placeholder: '邮箱和网址都可以不用填哟',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: true,
      path: window.location.pathname,
      master: '1ee9943f931df37af9dc1c57f5859da4',
    }

    if (true) { 
      initData.requiredFields= ('nick'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign({}, initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/js/mychange.js"></script><script defer src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/hexo-theme-volantis@latest/source/js/issues.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>